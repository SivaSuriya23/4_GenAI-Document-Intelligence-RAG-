{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c013b8a2-cdbe-4923-a14e-6ff97555d27a",
   "metadata": {},
   "source": [
    "### Loading Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1243217a-56b5-417e-a8eb-57308b8cf76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages loaded: 3\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "import os\n",
    "\n",
    "DATA_PATH = \"D:/My Projects/GenAI Document Intelligence (RAG)\"\n",
    "documents = []\n",
    "\n",
    "for file in os.listdir(DATA_PATH):\n",
    "    if file.endswith(\".docx\"):\n",
    "        loader = Docx2txtLoader(os.path.join(DATA_PATH, file))\n",
    "        docs = loader.load()\n",
    "        for d in docs:\n",
    "            d.metadata[\"source\"] = file\n",
    "        documents.extend(docs)\n",
    "\n",
    "print(f\"Total pages loaded: {len(documents)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c7e665-160e-4534-93ac-711f009826b3",
   "metadata": {},
   "source": [
    "### Text Chunking (Critical for Financial Docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba91a1fb-dde7-4d51-a784-11c7089242b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 1247\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=150\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Total chunks created: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e41dcec-0d2c-4938-80d6-73552c7ee027",
   "metadata": {},
   "source": [
    "### Load Hugging Face Embeddings (Offline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e28633b-28d9-4a71-b6c8-d8c717615bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuggingFace embeddings loaded\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "print(\"HuggingFace embeddings loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da813dbd-bb82-4e9d-a81b-0c1a1f7ada78",
   "metadata": {},
   "source": [
    "### Create FAISS Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3254b524-3935-4059-9d7b-f7930f0a43c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS vector store created successfully\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vector_db = FAISS.from_documents(chunks, embeddings)\n",
    "vector_db.save_local(\"faiss_index\")\n",
    "\n",
    "print(\"FAISS vector store created successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc17950-c77f-4039-869d-a9b20446273e",
   "metadata": {},
   "source": [
    "### Load Vector DB & Create Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af47b689-8c54-4bf1-b475-94be5761775c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = FAISS.load_local(\n",
    "    \"faiss_index\",\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "523001b9-269f-4891-8de5-5bf58d930c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever ready\n"
     ]
    }
   ],
   "source": [
    "retriever = vector_db.as_retriever(\n",
    "    search_kwargs={\"k\": 4}\n",
    ")\n",
    "\n",
    "print(\"Retriever ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73073918-9cf0-455e-ba04-331b2cb3fe15",
   "metadata": {},
   "source": [
    "### Add a Local / Free LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f008cd8-ca4d-4950-8c84-66b0e2657fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "C:\\Users\\Siva\\AppData\\Local\\Temp\\ipykernel_8856\\2665476009.py:10: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=pipe)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-base\",\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24814979-e3d6-4b20-8ef3-2e889ea29a0b",
   "metadata": {},
   "source": [
    "### Build Retrieval-Augmented Generation (RAG) Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b494806e-75ee-4549-b6c7-8209fdc685df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "768ec244-be47-48f7-9cc2-dbf4ebf34696",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the question using ONLY the context below.\n",
    "    If the answer is not in the context, say \"I don't know\".\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "    \"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99a82e3e-2586-4739-9c73-f916548e12d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runnable-based RAG pipeline ready\n"
     ]
    }
   ],
   "source": [
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"Runnable-based RAG pipeline ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7a0ef1-4ba5-4ac4-8651-b57c40d29975",
   "metadata": {},
   "source": [
    "### Ask Financial Questions (Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bad20125-6f03-4b58-ad19-ae639dd0a527",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3473 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: how does the company profited in 2022?\n",
      "A: We reported $198 billion in revenue and $83 billion in operating income. And the Microsoft Cloud surpassed $100 billion in annualized revenue for the first time.\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"how does the company profited in 2022?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    answer = rag_chain.invoke(q)\n",
    "    print(\"\\nQ:\", q)\n",
    "    print(\"A:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a76b61d7-8243-4397-b6c1-8a1a3ca3d205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What risks are mentioned in the FY22 10-K report?\n",
      "A: Quantitative and Qualitative Disclosures about Market Risk\n",
      "\n",
      "Q: What message did leadership emphasize in the shareholder letter?\n",
      "A: The importance of the effective engagement and action on environmental, social, and governance topics. To meet the expectations of our stakeholders and to and maintain their trust, we are committed to conducting our business in ways that are principled, transparent, and accountable and we have made a broad range of environmental and social commitments.\n",
      "\n",
      "Q: How did cloud services perform in FY2022?\n",
      "A: Whether we succeed in cloud-based services depends on our execution in several areas, including: nn Continuing to bring to market compelling cloud-based experiences that generate increasing traffic and market share. nnMaintaining the utility, compatibility, and performance of our cloud-based services on the growing array of computing devices, including PCs, smartphones, tablets, gaming consoles, and other devices, as well as sensors and other IoT endpoints. nn Continuing to enhance the attractiveness of our cloud platforms to third-party developers. nnEnsuring our cloud-based services meet the reliability expectations of our customers and maintain the security of their data as well as help them meet their own compliance needs. nnMaking our suite of cloud-based services platform-agnostic, available on a wide range of devices and ecosystems, including those of our competitors.\n",
      "\n",
      "Q: What were the major operating expenses?\n",
      "A: Research and development expenses increased $3.8 billion or 18% driven by investments in cloud engineering, Gaming, and LinkedIn. nnâ€¢tResearch and development expenses increased $3.8 billion or 18% driven by investments in cloud engineering, Gaming, and LinkedIn. nnâ€¢tSales and marketing expenses increased $1.7 billion or 8% driven by investments in commercial sales and LinkedIn. Sales and marketing included a favorable foreign currency impact of 2%. nnâ€¢tGeneral and administrative expenses increased $793 million or 16% driven by investments in corporate functions. nnOperating income increased $13.5 billion or 19% driven by growth across each of our segments.')\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What risks are mentioned in the FY22 10-K report?\",\n",
    "    \"What message did leadership emphasize in the shareholder letter?\",\n",
    "    \"How did cloud services perform in FY2022?\",\n",
    "    \"What were the major operating expenses?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    answer = rag_chain.invoke(q)\n",
    "    print(\"\\nQ:\", q)\n",
    "    print(\"A:\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2200ac51-da5c-44de-a591-7388f589e25d",
   "metadata": {},
   "source": [
    "### GRADIO APP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "79ef6e8e-0aea-439a-a33f-b2cfa07fbf1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7873\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7873/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (740 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# CONFIG\n",
    "# =====================================================\n",
    "DATA_PATH = \"D:/My Projects/GenAI Document Intelligence (RAG)\"\n",
    "FAISS_PATH = \"faiss_index\"\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Load Documents (same as notebook)\n",
    "# =====================================================\n",
    "documents = []\n",
    "\n",
    "for file in os.listdir(DATA_PATH):\n",
    "    if file.endswith(\".docx\"):\n",
    "        loader = Docx2txtLoader(os.path.join(DATA_PATH, file))\n",
    "        docs = loader.load()\n",
    "        for d in docs:\n",
    "            d.metadata[\"source\"] = file\n",
    "        documents.extend(docs)\n",
    "\n",
    "if not documents:\n",
    "    raise RuntimeError(\"No DOCX files found in DATA_PATH\")\n",
    "\n",
    "# =====================================================\n",
    "# Chunking\n",
    "# =====================================================\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=150\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Embeddings\n",
    "# =====================================================\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Vector Store\n",
    "# =====================================================\n",
    "if os.path.exists(FAISS_PATH):\n",
    "    vector_db = FAISS.load_local(\n",
    "        FAISS_PATH,\n",
    "        embeddings,\n",
    "        allow_dangerous_deserialization=True\n",
    "    )\n",
    "else:\n",
    "    vector_db = FAISS.from_documents(chunks, embeddings)\n",
    "    vector_db.save_local(FAISS_PATH)\n",
    "\n",
    "retriever = vector_db.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# LLM (Offline)\n",
    "# =====================================================\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-base\",\n",
    "    max_length=512\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Prompt\n",
    "# =====================================================\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the question using ONLY the context below.\n",
    "    If the answer is not in the context, say \"I don't know\".\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# RAG Chain (Runnable)\n",
    "# =====================================================\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Inference Function (SAFE)\n",
    "# =====================================================\n",
    "def ask_question(question: str):\n",
    "    question = str(question).strip()\n",
    "\n",
    "    if not question:\n",
    "        return \"Please enter a question.\", \"\"\n",
    "\n",
    "    try:\n",
    "        answer = rag_chain.invoke(question)\n",
    "\n",
    "        docs = retriever.invoke(question)\n",
    "        sources = sorted({d.metadata.get(\"source\", \"Unknown\") for d in docs})\n",
    "\n",
    "        sources_text = \"\\n\".join(f\"- {s}\" for s in sources)\n",
    "\n",
    "        return answer, sources_text\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error during inference: {str(e)}\", \"\"\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Gradio UI (STABLE)\n",
    "# =====================================================\n",
    "with gr.Blocks(title=\"GenAI Document Intelligence (RAG)\") as demo:\n",
    "\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # ðŸ“„ GenAI Document Intelligence (RAG)\n",
    "        Ask financial and business questions from Microsoft FY2022 documents  \n",
    "        using an **offline Retrieval-Augmented Generation system**.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    question_input = gr.Textbox(\n",
    "        label=\"Your Question\",\n",
    "        placeholder=\"e.g. How did cloud services perform in FY2022?\",\n",
    "        lines=2\n",
    "    )\n",
    "\n",
    "    ask_btn = gr.Button(\"Ask\")\n",
    "\n",
    "    answer_output = gr.Textbox(\n",
    "        label=\"Answer\",\n",
    "        lines=6\n",
    "    )\n",
    "\n",
    "    sources_output = gr.Textbox(\n",
    "        label=\"Sources\",\n",
    "        lines=4\n",
    "    )\n",
    "\n",
    "    ask_btn.click(\n",
    "        fn=ask_question,\n",
    "        inputs=question_input,\n",
    "        outputs=[answer_output, sources_output]\n",
    "    )\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d309cc-f00e-4003-a0df-725a63ac9c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b50cc2a-4ab4-47d0-8c21-90eab6ec5516",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
