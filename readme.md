# ğŸ“„ GenAI Document Intelligence (RAG)

## ğŸ“Œ Project Overview

**GenAI Document Intelligence (RAG)** is an **offline Retrieval-Augmented Generation (RAG) application** that enables users to ask natural-language questions and receive **concise, accurate answers** directly from unstructured business documents.

The system is designed to handle **financial reports, shareholder letters, and enterprise documents**, making it suitable for real-world use cases such as:

* Financial analysis
* Policy and compliance search
* Business intelligence
* Enterprise knowledge assistants

The application uses **semantic search + large language models** to ensure answers are grounded in the source documents and includes **source attribution** for transparency.

## ğŸ¯ Problem Statement

Traditional keyword-based document search:

* Misses semantic meaning
* Returns large, unstructured text blocks
* Requires manual reading

This project solves those limitations by:

* Converting documents into embeddings
* Retrieving only the most relevant context
* Generating **short, accurate, context-aware answers**

## ğŸš€ Key Features

* ğŸ” **Semantic Search** using FAISS vector database
* ğŸ§  **Retrieval-Augmented Generation (RAG)** pipeline
* âš¡ **Fast retrieval** with MMR (Maximal Marginal Relevance)
* ğŸ“„ **DOCX document ingestion**
* ğŸ“´ **Fully offline** (no OpenAI / API keys required)
* ğŸ§¾ **Source attribution** for every answer
* âœ‚ï¸ **Concise 2-line detailed answers**
* ğŸ–¥ï¸ **Interactive Gradio UI**
* â™»ï¸ **Answer caching** for instant repeated queries

## ğŸ—ï¸ System Architecture

```
DOCX Documents
      â†“
Document Loader (Docx2txt)
      â†“
Text Chunking (RecursiveCharacterTextSplitter)
      â†“
Sentence Embeddings (MiniLM)
      â†“
FAISS Vector Store
      â†“
MMR Retriever
      â†“
Prompt + LLM (FLAN-T5)
      â†“
Concise Answer + Sources
```

## ğŸ“ Project Structure

```
project/
â”‚
â”œâ”€â”€ app.py                 # Main Gradio application
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ README.md              # Project documentation
â”œâ”€â”€ faiss_index/           # Saved FAISS vector index
â”‚
â””â”€â”€ data/                  # Input documents
    â”œâ”€â”€ 2022_Annual_Report.docx
    â”œâ”€â”€ 2022_Shareholder_Letter.docx
    â””â”€â”€ MSFT_FY22Q4_10K.docx
```

## âš™ï¸ Technology Stack

| Component  | Technology                     |
| ---------- | ------------------------------ |
| UI         | Gradio                         |
| LLM        | Hugging Face FLAN-T5           |
| Embeddings | Sentence-Transformers (MiniLM) |
| Vector DB  | FAISS                          |
| Framework  | LangChain (Runnable APIs)      |
| Language   | Python                         |
| Deployment | Local / Hugging Face Spaces    |

## ğŸ§  Core Design Decisions

### 1ï¸âƒ£ Retrieval-Augmented Generation (RAG)

Instead of asking the LLM directly, the system:

* Retrieves relevant document chunks
* Injects them into the prompt
* Ensures answers are grounded in data

This **reduces hallucinations** and improves trust.

### 2ï¸âƒ£ FAISS + MMR Retrieval

* FAISS provides **fast similarity search**
* MMR reduces redundant chunks
* Smaller context â†’ faster inference â†’ better answers

### 3ï¸âƒ£ Prompt-Controlled Answer Length

Answers are constrained to:

* **Maximum 2 concise but informative lines**
* No unnecessary explanations
* Business-friendly output

### 4ï¸âƒ£ Offline-First Design

* No API keys
* No external calls
* Safe for enterprise environments

### 5ï¸âƒ£ Answer Caching

Repeated questions return **instant responses**, improving:

* Latency
* User experience
* System efficiency

## â–¶ï¸ Installation & Setup

### 1ï¸âƒ£ Create Virtual Environment (Recommended)

```bash
conda create -n rag python=3.10 -y
conda activate rag
```

### 2ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

## â–¶ï¸ Run the Application

```bash
python app.py
```

The Gradio interface will open at:

```
http://localhost:7860
```

## ğŸ§ª Example Questions

* What were Microsoftâ€™s key revenue drivers in FY2022?
* How did cloud services perform in FY2022?
* What risks are mentioned in the FY22 10-K report?
* What message did leadership emphasize in the shareholder letter?

## ğŸ“Š Performance Optimizations

| Optimization      | Benefit                   |
| ----------------- | ------------------------- |
| MMR Retrieval     | Faster & diverse context  |
| Answer Cache      | Instant repeat responses  |
| Small LLM         | Reduced latency           |
| Chunking Strategy | Better retrieval accuracy |

## ğŸ§  Skills Demonstrated

* Retrieval-Augmented Generation (RAG)
* Vector databases (FAISS)
* Semantic search & embeddings
* Prompt engineering
* LLM inference optimization
* Gradio UI development
* Production-safe GenAI system design

## ğŸ“Œ Project Description

**GenAI Document Intelligence (RAG)**
Developed an offline Retrieval-Augmented Generation system using LangChain, FAISS, and Hugging Face models to answer financial and business questions from unstructured documents. Implemented semantic search with MMR-based retrieval, optimized inference latency using caching, and deployed an interactive Gradio UI with concise, source-attributed responses.

## ğŸ§ª Limitations & Future Improvements

* Supports DOCX files only (PDF support can be added)
* Single-user local deployment
* No chat history (can be added safely)

**Future Enhancements**

* PDF ingestion
* Chat-style UI
* Hybrid BM25 + vector retrieval
* API deployment (FastAPI)

## ğŸ“œ License

This project is intended for **educational and portfolio demonstration purposes**.

## âœ… Final Notes

This project reflects **real-world GenAI system design**, not just a demo:

* Stable
* Explainable
* Offline